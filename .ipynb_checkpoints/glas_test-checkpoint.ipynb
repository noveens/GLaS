{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to do?\n",
    "\n",
    "#### Notation: \n",
    "- $\\mathbb{N}$: Number of training points\n",
    "- $\\mathbb{L}$: Number of labels\n",
    "- $BSZ$: Number of training points to load on GPU\n",
    "\n",
    "#### Input: \n",
    "- X features: $\\mathbb{X}_{i} ~ ~ \\forall i \\in \\{1, \\ldots, \\mathbb{N}\\}$\n",
    "- Correct label set: $\\mathbb{Y}_i ~ ~ \\forall i \\in \\{ 1, \\ldots, \\mathbb{N}\\}$\n",
    "\n",
    "#### Output: Learnt 1vA weights:\n",
    "- $\\mathbb{D}_{l} ~ ~ \\forall l \\in \\{1, \\ldots, \\mathbb{L}\\}$\n",
    "        \n",
    "#### Methodology:\n",
    "- Serialize all data\n",
    "    - $\\forall i \\in {1, \\ldots, \\mathbb{N}}$:\n",
    "        - $\\forall j \\in {1, \\ldots, Nonzero(\\mathbb{Y}_i})$:\n",
    "            - $FinalData$.add($\\{x_i, y_{i, j}\\}$)\n",
    "\n",
    "\n",
    "- GLaS regularizer preprocessing:\n",
    "    - Calculate $\\mathbb{G} \\leftarrow \\dfrac{AZ^{-1} + Z^{-1}A}{2}$ for all labels beforehand.\n",
    "    - Since some of the labels can have 0 occurances, we manually enforce the entire rows and cols in $\\mathbb{G}$ for all such labels to be 0\n",
    "\n",
    "\n",
    "- Training: \n",
    "    - Randomly Init $\\mathbb{D}_{l} ~ ~ \\forall l \\in \\mathbb{L}$ on GPU\n",
    "    - Transfer $\\mathbb{G}$ on GPU\n",
    "    \n",
    "    - Until convergence:\n",
    "        - Randomly shuffle $FinalData$ (To get different negatives)\n",
    "        - $\\forall i \\in \\{ 1, \\ldots, \\mathbb{N} ~ / ~ BSZ \\}$:\n",
    "            - $\\mathbb{X}_{Batch}, \\mathbb{Y}^{+}_{Batch}, \\mathbb{Y}^{-}_{Batch} \\ \\leftarrow loadBatch(i)$\n",
    "            - $\\hat{\\mathbb{Y}}_{Batch} \\leftarrow \\mathbb{D}_{Batch}^{T} \\cdot \\mathbb{X}_{Batch}$\n",
    "            - $\\mathbb{L}_{Batch} \\leftarrow RankingLoss(\\hat{\\mathbb{Y}}_{Batch}, \\mathbb{Y}^{+}_{Batch}, \\mathbb{Y}^{-}_{Batch}) + \\lambda \\cdot \\mathbb{L}_{GLaS}(\\mathbb{G}, \\mathbb{D})$\n",
    "            - $Backprop(\\mathbb{L}_{Batch})$\n",
    "\n",
    "\n",
    "- $loadBatch(i)$:\n",
    "    - $\\mathbb{X}_{Batch}, \\mathbb{Y}^{+}_{Batch} \\leftarrow FinalData[i : i+BSZ]$\n",
    "    - $\\mathbb{Y}_{temp} \\leftarrow \\bigcup{\\mathbb{Y}^{+}_{Batch}}$ // set\n",
    "    - $\\mathbb{Y}^{-}_{Batch} \\leftarrow [ ~ ]$ // list\n",
    "    - $\\forall i \\in \\{ 1, \\ldots, BSZ \\}$:\n",
    "        - $\\mathbb{Y}^{-}_{Batch}$.append($\\mathbb{Y}_{temp} - all\\_positives(\\mathbb{X}_{Batch}^{i})$)\n",
    "    - yeild $\\mathbb{X}_{Batch}, \\mathbb{Y}^{+}_{Batch}, \\mathbb{Y}^{-}_{Batch}$\n",
    "    \n",
    "    \n",
    "#### Results:\n",
    "- Eurlex-4k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (utils.py, line 97)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/t-nosach/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-90e2f7349c5c>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from utils import DataLoader, read_sparse\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/data/home/t-nosach/DiSMEC_GPU/GLaS/utils.py\"\u001b[0;36m, line \u001b[0;32m97\u001b[0m\n\u001b[0;31m    for i in pbar:\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from subprocess import PIPE, run\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import DataLoader, read_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"EURLex-4K\"\n",
    "\n",
    "DATA_DIR = dataset + \"/\"\n",
    "train_features_file = DATA_DIR + \"trn_X_Xf.txt\"\n",
    "train_labels_file = DATA_DIR + \"trn_X_Y.txt\"\n",
    "test_features_file = DATA_DIR + \"tst_X_Xf.txt\"\n",
    "test_labels_file = DATA_DIR + \"tst_X_Y.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data files..\")\n",
    "train_x = DataLoader(train_features_file, train_labels_file)\n",
    "num_train_points = train_x.num_points\n",
    "num_labels = train_x.num_labels\n",
    "vocab_x = train_x.num_words\n",
    "\n",
    "test_x = DataLoader(test_features_file, test_labels_file)\n",
    "num_test_points = test_x.num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_log_file_path(hyper_params):\n",
    "    log_file  = 'logs/stochastic_' + hyper_params['dataset']\n",
    "    log_file += '_embed_' + str(hyper_params['embedding_dim'])\n",
    "    log_file += '_bsz_' + str(hyper_params['num_data_to_load_on_GPU'])\n",
    "    log_file += '_lamda_' + str(hyper_params['lamda'])\n",
    "    log_file += '_dropout_' + str(hyper_params['dropout'])\n",
    "    log_file += '_lr_' + str(hyper_params['lr'])\n",
    "    log_file += '_tf_' + str(hyper_params['tf'])\n",
    "    log_file += '.txt'\n",
    "    \n",
    "    return log_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(score_indices, scores, train_file = False):\n",
    "    scores_new = scores.data.cpu().numpy().tolist()\n",
    "\n",
    "    score_indices_new = []\n",
    "    for i in range(score_indices.shape[0]): score_indices_new.append(score_indices[i].data.cpu().numpy().tolist())\n",
    "\n",
    "    # Save score matrix to file\n",
    "    out_str = str(scores.shape[0]) + \" \" + str(num_labels) + '\\n'\n",
    "    for point_num in range(len(scores)):\n",
    "        t1 = (list(map(str, score_indices_new[point_num])))\n",
    "        t2 = (list(map(str, scores_new[point_num])))\n",
    "        t3 = [ t1[i] + \":\" + t2[i] for i in range(len(t1)) ]\n",
    "        out_str += ' '.join(t3) + '\\n'\n",
    "    f = open(\"score_mat_new.txt\", \"w\")\n",
    "    f.write(out_str)\n",
    "    f.close()\n",
    "\n",
    "    # Compute metrics\n",
    "    if train_file == True: print(\"Computing train metrics..\")\n",
    "    else: print(\"Computing test metrics..\")\n",
    "\n",
    "    command = \"python ../../Slice/Tools/python/metrics/calc_metrics.py --score_file score_mat_new.txt --trn_X_Y \" \n",
    "    command += train_labels_file \n",
    "    \n",
    "    command += \" --tst_X_Y \"\n",
    "    if train_file == False: command += test_labels_file\n",
    "    else: command += train_labels_file\n",
    "    \n",
    "    if dataset in [ \"AmazonCat-13K\", \"AmazonTitles-300K\" ]: command += \" -A 0.6 -B 2.6\"\n",
    "    elif dataset in [ \"Wiki10\", \"Wiki-500K\", \"WikiTitles-500K\" ]: command += \" -A 0.5 -B 0.4\"\n",
    "    elif dataset in [ \"EURLex-4K\" ]: command += \" -A 0.55 -B 1.5\"\n",
    "\n",
    "    result = run(command, stdout=PIPE, stderr=PIPE, universal_newlines=True, shell=True)\n",
    "    \n",
    "    metrics = result.stdout\n",
    "    print(\"\\n\".join(metrics.split(\"\\n\")[1:3]), \"\\n\")\n",
    "    \n",
    "    return result.stdout\n",
    "    \n",
    "def evaluate(hyper_params, reader):\n",
    "    \n",
    "    # Initializing\n",
    "    score_indices = torch.cuda.LongTensor(num_test_points, hyper_params['num_to_save'])\n",
    "    scores = torch.cuda.FloatTensor(num_test_points, hyper_params['num_to_save'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, offsets, tf, data_batch, bar in reader.iter_eval(\n",
    "            bsz = hyper_params['num_data_to_load_on_GPU'],\n",
    "            tf = hyper_params['tf']\n",
    "        ):\n",
    "\n",
    "            # Forward\n",
    "            scores_batch = model(x, offsets, tf)\n",
    "\n",
    "            # Storing top-k indices\n",
    "            vals, inds = torch.topk(scores_batch, k = hyper_params['num_to_save'], sorted = True)\n",
    "            scores[data_batch : data_batch + hyper_params['num_data_to_load_on_GPU'], :] = vals\n",
    "            score_indices[data_batch : data_batch + hyper_params['num_data_to_load_on_GPU'], :] = inds\n",
    "    \n",
    "    # Metrics\n",
    "    return get_metrics(score_indices, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_coocc(label_file):\n",
    "    [ rows, cols, data ], nr, nc = read_sparse(label_file, return_separate = True)\n",
    "    \n",
    "    # [ num_points x num_labels ]\n",
    "    matrix = csr_matrix((data, (rows, cols)), shape = (nr, nc), dtype = np.float32)\n",
    "    return (matrix.T @ matrix).todense()\n",
    "\n",
    "def get_G(train_labels_file):\n",
    "    # Computing label coocc\n",
    "    label_cooccurence = torch.cuda.FloatTensor(\n",
    "        get_label_coocc(train_labels_file)\n",
    "    )\n",
    "    num_labels = label_cooccurence.shape[1]\n",
    "    \n",
    "    # Getting zinv\n",
    "    diag = torch.diag(label_cooccurence)\n",
    "    diag[diag < 0.001] = 0.1 # For labels occuring zero times\n",
    "    inv_diag = 1.0 / diag\n",
    "    inv_diag[inv_diag > 1.0] = 0.0 # Setting labels occuring zero times to be zero\n",
    "    \n",
    "    ind = np.diag_indices(num_labels)\n",
    "    zinv = torch.zeros(num_labels, num_labels).cuda()\n",
    "    zinv[ind[0], ind[1]] = inv_diag # Will set only the diagonal\n",
    "\n",
    "    # Getting azinv\n",
    "    a_zinv = torch.matmul(label_cooccurence, zinv)\n",
    "\n",
    "    # Getting `G`\n",
    "    return Variable(0.5 * (a_zinv + a_zinv.t()), requires_grad = False)\n",
    "\n",
    "# Pre-computing `G` for all labels beforehand\n",
    "G = get_G(train_labels_file)\n",
    "\n",
    "def get_glass_reg(w_t, all_ys):    \n",
    "    global G\n",
    "    \n",
    "    # Normalize label embeddings to be unit norm \n",
    "    # since `G` has diagonal entries 1 (by construction)\n",
    "    # Shape of `w_t`: [ features x num_labels ]\n",
    "    w_t = w_t / torch.norm(w_t, dim = 0).unsqueeze(0)\n",
    "    \n",
    "    # Selecting only those labels which are active in this batch\n",
    "    w_t_batch = w_t[:, all_ys]\n",
    "    G_batch = G[all_ys, :][:, all_ys]\n",
    "    \n",
    "    glas  = torch.matmul(w_t_batch.t(), w_t_batch)\n",
    "    glas -= G_batch\n",
    "    \n",
    "    return torch.sum(glas ** 2) #/ (float(w_t_batch.shape[1]) ** 2)\n",
    "\n",
    "def compute_loss_multilabel(output, y, negs):\n",
    "    pos_scores = output.gather(-1, y).unsqueeze(1)\n",
    "    \n",
    "    # Padded elements in `negs` are padded with `num_labels`\n",
    "    # Pad last dimension with -INF so that padded elements in `y` get -INF\n",
    "    output = F.pad(output, pad=(0, 1), mode=\"constant\", value = -float(1e5))\n",
    "    neg_scores = output.gather(-1, negs).unsqueeze(-1)\n",
    "    output = output[:, :-1]\n",
    "    \n",
    "    # Since we have -INF for padded elements in `neg_scores`\n",
    "    # taking max with 0.0 would reduce them to zero\n",
    "    loss = torch.clamp(1.0 + neg_scores - pos_scores, min = 0.0)\n",
    "    \n",
    "    # shape of `loss` should be [bsz x neg x pos]\n",
    "    # We take sum for all pos, negs\n",
    "    return torch.mean(torch.sum(torch.sum(loss, -1), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'dataset': dataset,\n",
    "    \n",
    "    'num_data_to_load_on_GPU': int(256), # BSZ\n",
    "    'embedding_dim': 1024, # Word & label embedding dimension\n",
    "    'dropout': 0.2, # Word dropout\n",
    "    'tf': False, # Whether to use TF-IDF weighted sum of embeddings\n",
    "    \n",
    "    'num_to_save': 5, # These many predictions will be saved for computing metrics\n",
    "}\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hyper_params, vocab_x, num_labels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        self.word_embedding_matrix = nn.EmbeddingBag(\n",
    "            vocab_x, hyper_params['embedding_dim'], mode='sum', \n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.word_embedding_matrix.weight.data)\n",
    "        \n",
    "        self.linear1 = nn.utils.weight_norm(\n",
    "            nn.Linear(hyper_params['embedding_dim'], hyper_params['embedding_dim']),\n",
    "            name = 'weight'\n",
    "        )\n",
    "        self.linear2 = nn.utils.weight_norm(\n",
    "            nn.Linear(hyper_params['embedding_dim'], num_labels, bias = False), \n",
    "            name = 'weight'\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.linear1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight.data)\n",
    "                \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_val = hyper_params['dropout']\n",
    "\n",
    "    def embed(self, x, offsets, tf):\n",
    "        if self.hyper_params['tf'] == True:\n",
    "            return self.word_embedding_matrix(x, offsets, per_sample_weights = tf)\n",
    "        \n",
    "        return self.word_embedding_matrix(x, offsets)\n",
    "        \n",
    "    def forward(self, x, offsets, tf):\n",
    "        # Get document embeddings\n",
    "        temp = self.relu(self.embed(x, offsets, tf)) # Non-linear\n",
    "        temp = self.linear1(temp) # Linear\n",
    "        \n",
    "        # Calculate scores\n",
    "        temp = self.linear2(temp)\n",
    "        \n",
    "        return temp\n",
    "\n",
    "print(\"Training..\")\n",
    "start_time = time.time()\n",
    "\n",
    "for bsz in [ 256 ]:\n",
    "    \n",
    "    hyper_params['num_data_to_load_on_GPU'] = bsz\n",
    "    \n",
    "    for tf in [ False, True ]:\n",
    "        \n",
    "        hyper_params['tf'] = tf\n",
    "        \n",
    "        if hyper_params['tf'] == True: hyper_params['lr'] = float(5e-5)\n",
    "        else: hyper_params['lr'] = float(1e-3)\n",
    "        \n",
    "        for lamda in [ 0.0, 1.0, 10.0, float(1e2), float(1e3) ]:\n",
    "            \n",
    "            hyper_params['lamda'] = lamda\n",
    "            \n",
    "            # Log file path\n",
    "            hyper_params['log_file'] = get_log_file_path(hyper_params)\n",
    "            \n",
    "            # Clear log file\n",
    "            f = open(hyper_params['log_file'], \"w\")\n",
    "            f.write(\"\")\n",
    "            f.close()\n",
    "            \n",
    "            # Initializing score matrices for metric computation\n",
    "            score_indices = torch.cuda.LongTensor(num_train_points, hyper_params['num_to_save'])\n",
    "            scores = torch.cuda.FloatTensor(num_train_points, hyper_params['num_to_save'])\n",
    "            \n",
    "            # Initializing model and optim.\n",
    "            model = Net(hyper_params, vocab_x, num_labels).cuda(); print(model)\n",
    "            optim = torch.optim.SGD(model.parameters(), lr = hyper_params['lr'], momentum=0.9)\n",
    "            crit  = compute_loss_multilabel\n",
    "            \n",
    "            # Training\n",
    "            for epoch in range(1):\n",
    "                hyper_params['epoch'] = epoch + 1\n",
    "                print(\"Running epoch:\", hyper_params['epoch'])\n",
    "\n",
    "                av_glass = 0.0; av_loss = 0.0; done = 0.0\n",
    "\n",
    "                for x, offsets, tf, y, all_ys, negs, pbar in train_x.iter(\n",
    "                    dropout = hyper_params['dropout'], \n",
    "                    bsz = hyper_params['num_data_to_load_on_GPU'],\n",
    "                    tf = hyper_params['tf']\n",
    "                ):\n",
    "\n",
    "                    scores_for_all_labels = None\n",
    "                    def closure():\n",
    "                        global scores_for_all_labels, av_glass\n",
    "                        optim.zero_grad()\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        scores_for_all_labels = model(x, offsets, tf)\n",
    "                        \n",
    "                        # Loss computation\n",
    "                        loss = crit(scores_for_all_labels, y, negs)\n",
    "                        glass_reg = get_glass_reg(model.linear2.weight.t(), all_ys)\n",
    "                        av_glass += float(glass_reg)\n",
    "                        loss += (lamda) * glass_reg\n",
    "                        \n",
    "                        # Backward\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                    \n",
    "                    # Optimizing\n",
    "                    loss = optim.step(closure)\n",
    "\n",
    "                    # Storing loss\n",
    "                    av_loss += float(loss.data); done += 1.0\n",
    "                    \n",
    "                    # Updating TQDM\n",
    "                    pbar.set_description(\n",
    "                        \"L: \" + str(lamda) + \\\n",
    "                        \", G: \" + str(round(av_glass / done, 4)) + \\\n",
    "                        \", Total: \" + str(round(av_loss / done, 4))\n",
    "                    )\n",
    "\n",
    "                # Computing metrics on test set at epoch end\n",
    "                with torch.no_grad(): metrics = evaluate(hyper_params, test_x)\n",
    "                \n",
    "                # Logging at epoch end\n",
    "                f = open(hyper_params['log_file'], \"a\")\n",
    "                f.write(\"Epoch #\" + str(hyper_params['epoch']) + \":\\n\")\n",
    "                f.write(\"Lamda:\" + str(lamda) + \", Glass: \" + str(round(av_glass / done, 4)) + \", Total loss: %8f\" % (av_loss / done) + \"\\n\")\n",
    "                f.write(metrics + \"\\n\")\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "font = {\n",
    "    'family' : 'DejaVu Sans',\n",
    "    'weight' : 'normal',\n",
    "    'size'   : 26\n",
    "}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "metrics = [ 'Pk', 'PSPk' ]\n",
    "lamdas = [ 0.0, 1.0, 10.0, float(1e2), float(1e3) ]\n",
    "\n",
    "tf = False\n",
    "\n",
    "fig, plts = plt.subplots(2, 2, figsize = (18, 16))\n",
    "\n",
    "for at, tf in enumerate([ False, True ]):\n",
    "    \n",
    "    hyper_params['tf'] = tf\n",
    "    \n",
    "    x = []; y = {}\n",
    "    for m in metrics: y[m] = { '1': [], '3': [], '5': [] }\n",
    "        \n",
    "    plt = plts[at]\n",
    "    \n",
    "    for lamda in lamdas:\n",
    "        x.append(lamda + 1) # For when lamda = 0.0\n",
    "        \n",
    "        hyper_params['lamda'] = lamda\n",
    "\n",
    "        if tf == True: hyper_params['lr'] = float(5e-5)\n",
    "        else: hyper_params['lr'] = float(1e-3)\n",
    "\n",
    "        log_file = get_log_file_path(hyper_params)\n",
    "        f = open(log_file, \"r\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        y_temp = {}\n",
    "        for m in metrics: y_temp[m] = { '1': [], '3': [], '5': [] }\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            if line[:5] == \"Epoch\":\n",
    "\n",
    "                for j in range(i+3, i+9):\n",
    "\n",
    "                    line = lines[j].strip()\n",
    "\n",
    "                    if line.split(\":\")[0] in metrics:\n",
    "\n",
    "                        y_temp[line.split(\":\")[0]]['1'].append(\n",
    "                            float(line.split(\":\")[2].split()[0].strip())\n",
    "                        )\n",
    "                        y_temp[line.split(\":\")[0]]['3'].append(\n",
    "                            float(line.split(\":\")[3].split()[0].strip())\n",
    "                        )\n",
    "                        y_temp[line.split(\":\")[0]]['5'].append(\n",
    "                            float(line.split(\":\")[4].strip())\n",
    "                        )\n",
    "\n",
    "        for m in metrics:\n",
    "            try: y[m]['1'].append(max(y_temp[m]['1']))\n",
    "            except: y[m]['1'].append(0.0)\n",
    "                \n",
    "            try: y[m]['3'].append(max(y_temp[m]['3']))\n",
    "            except: y[m]['3'].append(0.0)\n",
    "                \n",
    "            try: y[m]['5'].append(max(y_temp[m]['5']))\n",
    "            except: y[m]['5'].append(0.0)\n",
    "\n",
    "    plt[0].plot(x, y['Pk']['1'])\n",
    "    plt[0].plot(x, y['Pk']['3'])\n",
    "    plt[0].plot(x, y['Pk']['5'])\n",
    "    plt[1].plot(x, y['PSPk']['1'])\n",
    "    plt[1].plot(x, y['PSPk']['3'])\n",
    "    plt[1].plot(x, y['PSPk']['5'])\n",
    "    \n",
    "    for i in range(2):\n",
    "        if at == 1: plt[i].set_xlabel(\"$\\lambda$\")\n",
    "        plt[i].set_xticks(x)\n",
    "        plt[i].set_xscale('log')\n",
    "        plt[i].set_title(\"TF-IDF = \" + str(tf))\n",
    "    plt[0].set_ylabel(\"P\")\n",
    "    plt[1].set_ylabel(\"PSP\")\n",
    "\n",
    "leg = [\"@1\", \"@3\", \"@5\"]\n",
    "fig.legend(leg, ncol = 3)\n",
    "fig.subplots_adjust(top = 0.9)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [ 'Pk', 'PSPk' ]\n",
    "lamdas = [ 0.0, 1.0, 10.0, 100.0, float(1e3) ]\n",
    "\n",
    "fig, plts = plt.subplots(len(metrics) * 2, len(lamdas), figsize = (20, 20))\n",
    "\n",
    "for at_tf, tf in enumerate([ False, True ]):\n",
    "    \n",
    "    hyper_params['tf'] = tf\n",
    "    \n",
    "    for at2, lamda in enumerate(lamdas):\n",
    "        \n",
    "        hyper_params['lamda'] = lamda\n",
    "\n",
    "        if tf == True: hyper_params['lr'] = float(5e-5)\n",
    "        else: hyper_params['lr'] = float(1e-3)\n",
    "\n",
    "        log_file = get_log_file_path(hyper_params)\n",
    "        f = open(log_file, \"r\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        x = []; y = {}\n",
    "        for m in metrics: y[m] = { '1': [], '3': [], '5': [] }\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            if line[:5] == \"Epoch\":\n",
    "\n",
    "                check = False\n",
    "                for j in range(i+3, i+9):\n",
    "                        \n",
    "                    if j >= len(lines): continue\n",
    "                    line = lines[j].strip()\n",
    "\n",
    "                    if line.split(\":\")[0] in metrics:\n",
    "                        check = True\n",
    "\n",
    "                        y[line.split(\":\")[0]]['1'].append(\n",
    "                            float(line.split(\":\")[2].split()[0].strip())\n",
    "                        )\n",
    "                        y[line.split(\":\")[0]]['3'].append(\n",
    "                            float(line.split(\":\")[3].split()[0].strip())\n",
    "                        )\n",
    "                        y[line.split(\":\")[0]]['5'].append(\n",
    "                            float(line.split(\":\")[4].strip())\n",
    "                        )\n",
    "\n",
    "                if check == True: x.append(len(x) + 1)\n",
    "\n",
    "        for at, m in enumerate(metrics):\n",
    "            plts[at + (at_tf * len(metrics))][at2].plot(x, y[m]['1'])\n",
    "            plts[at + (at_tf * len(metrics))][at2].plot(x, y[m]['3'])\n",
    "            plts[at + (at_tf * len(metrics))][at2].plot(x, y[m]['5'])\n",
    "            \n",
    "            if at == len(metrics) - 1 and at_tf == 1:\n",
    "                plts[at + (at_tf * len(metrics))][at2].set_xlabel(\"Epochs\")\n",
    "            \n",
    "            title = \"$\\lambda$ = \" + str(lamda) + \", \" + metrics[at]\n",
    "            if tf == True: title += ', TF-IDF'\n",
    "            plts[at + (at_tf * len(metrics))][at2].set_title(title)\n",
    "\n",
    "            if m == 'Pk': plts[at + (at_tf * len(metrics))][at2].set_ylim(40, 80)\n",
    "            elif m == 'PSPk': plts[at + (at_tf * len(metrics))][at2].set_ylim(20, 50)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
